{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfIgCJ4o_qnO",
        "outputId": "52dc1ece-7c3d-42fb-eabb-b365c82eff2c"
      },
      "outputs": [],
      "source": [
        "!pip install kagglehub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "-dhd6KQw_uc4",
        "outputId": "1b8892d1-6310-4a72-a79d-221fd78fe7d6"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()   # Upload your kaggle.json file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIxWXYC1_uC9"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv \"kaggle (1).json\" ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rFGovou_rgF",
        "outputId": "e04045c5-5f81-440b-cdb7-26cea3c14a29"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d divg07/casia-20-image-tampering-detection-dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXZE_ozSAIiI",
        "outputId": "03cc5c25-da6d-4463-8768-01ec0af46d9c"
      },
      "outputs": [],
      "source": [
        "!unzip casia-20-image-tampering-detection-dataset.zip -d casia_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVK-SmKPALQ4",
        "outputId": "bd810090-c69b-487d-a2ab-0f49183e1b15"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "for root, dirs, files in os.walk(\"casia_dataset\"):\n",
        "    print(root, len(files), \"files\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lN8ggibANdj",
        "outputId": "bbac61e4-86b6-4ee7-dd96-28fe947fd6fb"
      },
      "outputs": [],
      "source": [
        "!pip install timm matplotlib opencv-python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMPpDefVAP4N",
        "outputId": "d7458bf3-a0c8-4aae-8264-c24a677b6a86"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "data_dir = \"/content/casia_dataset/CASIA2\"\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    root=data_dir,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "print(\"Classes:\", train_dataset.classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KbVTAzqASbe",
        "outputId": "4c380380-4293-4fbe-8f07-97d98a6abb12"
      },
      "outputs": [],
      "source": [
        "# Filter out _masks folder\n",
        "full_dataset = datasets.ImageFolder(root=data_dir, transform=None)\n",
        "# Remove _masks from classes\n",
        "filtered_samples = [(p,l) for p,l in full_dataset.samples if full_dataset.classes[l] != \"_masks\"]\n",
        "paths = [p for p,_ in filtered_samples]\n",
        "labels = [l for _,l in filtered_samples]\n",
        "\n",
        "print(\"Filtered dataset samples:\", len(paths))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "20d9040e22ef4d46998fe6e372f07184",
            "a0ac1fd1f51c49a2b0f9f9feaa0324b6",
            "a8cd2d9f24964fe3b25829792dda9153",
            "926e89829b15438bb88bf8d679641be1",
            "775e993ee66d4c1197bffad63f8549aa",
            "cd3e476217924e34892de50018034397",
            "d4db3e720e1f4018a3e517553a5ac21a",
            "f90d27bd74a34fe39f9409b06ba6548a",
            "72db44d8e4f34215a44e1dbc4df41ae4",
            "36ae40ec55574fd190d0cd4a0e933c4e",
            "65288f656f27476a9aaa6ca4d18e7dd1"
          ]
        },
        "id": "vhM1aAJLAVos",
        "outputId": "b34f6daf-8401-4719-f6b9-3ba952aaf501"
      },
      "outputs": [],
      "source": [
        "import timm\n",
        "import torch.nn as nn\n",
        "\n",
        "model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "model.head = nn.Linear(model.head.in_features, 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mQEBP4QAYjX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG2M7EVcAbNN",
        "outputId": "f5fdc56b-5daf-40bb-a643-727d869280f8"
      },
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Imports & Config\n",
        "# -------------------------\n",
        "import os, random, shutil\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import timm\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "# -------------------------\n",
        "# Basic settings\n",
        "# -------------------------\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# Dataset path\n",
        "data_dir = \"/content/casia_dataset/CASIA2\"  # Only Au & Tp should remain\n",
        "\n",
        "# -------------------------\n",
        "# Remove Groundtruth masks if still present\n",
        "# -------------------------\n",
        "mask_dir = os.path.join(data_dir, \"CASIA 2 Groundtruth\")\n",
        "if os.path.exists(mask_dir):\n",
        "    shutil.move(mask_dir, os.path.join(data_dir, \"_masks\"))\n",
        "    print(\"Moved Groundtruth masks out of training path\")\n",
        "\n",
        "# -------------------------\n",
        "# Hyperparameters\n",
        "# -------------------------\n",
        "num_epochs = 12\n",
        "batch_size = 24\n",
        "img_size = 224\n",
        "num_classes = 2\n",
        "best_val_f1 = 0.0\n",
        "\n",
        "# -------------------------\n",
        "# Transforms\n",
        "# -------------------------\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(img_size, scale=(0.7,1.0), ratio=(0.9,1.1)),\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.3, hue=0.05),\n",
        "    transforms.RandAugment(num_ops=2, magnitude=9),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomErasing(p=0.25, scale=(0.02, 0.2), ratio=(0.3, 3.3), inplace=False),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(int(img_size*1.12)),\n",
        "    transforms.CenterCrop(img_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        " # -------------------------\n",
        "# Dataset & Dataloaders (filtered)\n",
        "# -------------------------\n",
        "full_dataset = datasets.ImageFolder(root=data_dir, transform=None)\n",
        "\n",
        "# Filter out _masks folder\n",
        "filtered_samples = [(p,l) for p,l in full_dataset.samples if full_dataset.classes[l] != \"_masks\"]\n",
        "paths = [p for p,_ in filtered_samples]\n",
        "labels = [l for _,l in filtered_samples]\n",
        "\n",
        "# Stratified train/val split\n",
        "from collections import defaultdict\n",
        "by_label = defaultdict(list)\n",
        "for p,l in zip(paths, labels):\n",
        "    by_label[l].append(p)\n",
        "\n",
        "train_paths, train_labels, val_paths, val_labels = [], [], [], []\n",
        "val_ratio = 0.15\n",
        "\n",
        "for l, items in by_label.items():\n",
        "    random.shuffle(items)\n",
        "    n_val = max(1, int(len(items) * val_ratio))\n",
        "    val_items = items[:n_val]\n",
        "    train_items = items[n_val:]\n",
        "    train_paths += train_items\n",
        "    train_labels += [l]*len(train_items)\n",
        "    val_paths += val_items\n",
        "    val_labels += [l]*len(val_items)\n",
        "\n",
        "print(\"Train samples:\", len(train_paths))\n",
        "print(\"Val samples:\", len(val_paths))\n",
        "\n",
        "# Custom Dataset\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class CASIADataset(Dataset):\n",
        "    def __init__(self, paths, labels, transform=None):\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, self.labels[idx], self.paths[idx]\n",
        "\n",
        "train_ds = CASIADataset(train_paths, train_labels, transform=train_transform)\n",
        "val_ds   = CASIADataset(val_paths, val_labels, transform=val_transform)\n",
        "\n",
        "# DataLoaders (reduce workers to 2 for Colab)\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "\n",
        "# Model: ViT-B/16\n",
        "# -------------------------\n",
        "model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "if hasattr(model, 'head'):\n",
        "    model.head = nn.Linear(model.head.in_features, num_classes)\n",
        "else:\n",
        "    model.reset_classifier(num_classes=num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "# -------------------------\n",
        "# Optimizer, Scheduler, Loss\n",
        "# -------------------------\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scaler = torch.amp.GradScaler()\n",
        "\n",
        "# -------------------------\n",
        "# Metrics\n",
        "# -------------------------\n",
        "def epoch_metrics_all(preds, labels):\n",
        "    preds = np.asarray(preds)\n",
        "    labels = np.asarray(labels)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    p,r,f1,_ = precision_recall_fscore_support(labels, preds, average='binary', zero_division=0)\n",
        "    return acc,p,r,f1\n",
        "\n",
        "# -------------------------\n",
        "# Training & Validation\n",
        "# -------------------------\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device, scaler):\n",
        "    model.train()\n",
        "    losses, all_preds, all_labels = [], [], []\n",
        "    for imgs, labels, _ in tqdm(loader, desc=\"Train\", leave=False):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.amp.autocast(device_type='cuda'):\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        losses.append(loss.item())\n",
        "        all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "    mean_loss = float(np.mean(losses))\n",
        "    return mean_loss, *epoch_metrics_all(all_preds, all_labels)\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    losses, all_preds, all_labels = [], [], []\n",
        "    for imgs, labels, _ in tqdm(loader, desc=\"Val\", leave=False):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        with torch.amp.autocast(device_type='cuda'):\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "        losses.append(loss.item())\n",
        "        all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "    mean_loss = float(np.mean(losses))\n",
        "    return mean_loss, *epoch_metrics_all(all_preds, all_labels)\n",
        "\n",
        "\n",
        "def show_img_and_heatmap(img_path, cam_map, alpha=0.5):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = cv2.resize(img, (img_size, img_size))\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255*cam_map), cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "    overlay = np.uint8(heatmap*alpha + img*(1-alpha))\n",
        "    fig, ax = plt.subplots(1,3, figsize=(12,4))\n",
        "    ax[0].imshow(img); ax[0].set_title(\"Image\"); ax[0].axis('off')\n",
        "    ax[1].imshow(heatmap); ax[1].set_title(\"Heatmap\"); ax[1].axis('off')\n",
        "    ax[2].imshow(overlay); ax[2].set_title(\"Overlay\"); ax[2].axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# -------------------------\n",
        "# Training loop with checkpoint\n",
        "# -------------------------\n",
        "save_dir = Path(\"/content/checkpoints_casia_vit\")\n",
        "save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for epoch in range(1,num_epochs+1):\n",
        "    print(f\"\\n=== Epoch {epoch}/{num_epochs} ===\")\n",
        "    train_loss, train_acc, train_p, train_r, train_f1 = train_one_epoch(model, train_loader, optimizer, criterion, device, scaler)\n",
        "    val_loss, val_acc, val_p, val_r, val_f1 = validate(model, val_loader, criterion, device)\n",
        "    scheduler.step()\n",
        "    print(f\"Train: loss {train_loss:.4f} acc {train_acc:.4f} p {train_p:.4f} r {train_r:.4f} f1 {train_f1:.4f}\")\n",
        "    print(f\" Val : loss {val_loss:.4f} acc {val_acc:.4f} p {val_p:.4f} r {val_r:.4f} f1 {val_f1:.4f}\")\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        ckpt_path = save_dir / f\"best_vit_epoch{epoch}_f1{val_f1:.4f}.pt\"\n",
        "        torch.save({'epoch':epoch,'model_state_dict':model.state_dict(),\n",
        "                    'optimizer_state_dict':optimizer.state_dict(),'val_f1':val_f1}, ckpt_path)\n",
        "        print(\"Saved best checkpoint:\", ckpt_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SetHNNehAd2w"
      },
      "outputs": [],
      "source": [
        "# ===========================================\n",
        "# üîß Stable Grad-CAM Hooks for ViT-B/16 (timm)\n",
        "# ===========================================\n",
        "activations, gradients = None, None\n",
        "\n",
        "# Hook the attention output layer (strong gradient signal)\n",
        "target_layer = model.blocks[-1].attn  # works for all timm ViT versions\n",
        "\n",
        "def forward_hook(module, input, output):\n",
        "    global activations\n",
        "    activations = output.detach()\n",
        "    activations.requires_grad_(True)\n",
        "\n",
        "def backward_hook(module, grad_in, grad_out):\n",
        "    global gradients\n",
        "    gradients = grad_out[0].detach()\n",
        "\n",
        "hook_fwd = target_layer.register_forward_hook(forward_hook)\n",
        "hook_bwd = target_layer.register_full_backward_hook(backward_hook)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHdqAgltMbG2"
      },
      "outputs": [],
      "source": [
        "def compute_vit_gradcam(model, input_tensor, target_class=None):\n",
        "    global activations, gradients\n",
        "    model.eval()\n",
        "    activations, gradients = None, None\n",
        "\n",
        "    input_tensor = input_tensor.to(device)\n",
        "    logits = model(input_tensor)\n",
        "    pred_class = logits.argmax(dim=1).item() if target_class is None else int(target_class)\n",
        "    score = logits[0, pred_class]\n",
        "\n",
        "    model.zero_grad()\n",
        "    score.backward(retain_graph=True)\n",
        "\n",
        "    # Safety checks\n",
        "    if activations is None or gradients is None:\n",
        "        raise RuntimeError(\"Hooks didn't capture activations or gradients!\")\n",
        "\n",
        "    acts = activations[0]       # (tokens, C)\n",
        "    grads = gradients[0]\n",
        "\n",
        "    # Remove CLS token if present\n",
        "    if acts.shape[0] > 1:\n",
        "        acts = acts[1:]\n",
        "        grads = grads[1:]\n",
        "\n",
        "    # Compute weights (mean gradients)\n",
        "    weights = grads.mean(dim=0)\n",
        "\n",
        "    # Weighted combination\n",
        "    cam = (acts * weights.unsqueeze(0)).sum(dim=1).detach().cpu().numpy()\n",
        "\n",
        "    # Reshape to patch grid (auto-detect)\n",
        "    grid_size = int(np.sqrt(cam.shape[0]))\n",
        "    cam = cam.reshape(grid_size, grid_size)\n",
        "    cam = np.maximum(cam, 0)\n",
        "\n",
        "    # Normalize [0,1]\n",
        "    cam -= cam.min()\n",
        "    if cam.max() != 0:\n",
        "        cam /= cam.max()\n",
        "\n",
        "    # Resize to full image resolution\n",
        "    cam = cv2.resize(cam, (input_tensor.shape[-1], input_tensor.shape[-2]))\n",
        "    return cam, pred_class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fRuO0Z9NMglF",
        "outputId": "fb308d1c-801a-4b70-b3b9-ad972fe83d8d"
      },
      "outputs": [],
      "source": [
        "class_names = ['Au', 'Tp']\n",
        "examples = [(p, l) for p, l in zip(val_paths, val_labels)]\n",
        "random.shuffle(examples)\n",
        "examples = examples[:6]\n",
        "\n",
        "for p, l in examples:\n",
        "    im = val_transform(Image.open(p).convert(\"RGB\")).unsqueeze(0)\n",
        "    cam_map, pred = compute_vit_gradcam(model, im)\n",
        "    print(f\"GT: {class_names[l]} | Pred: {class_names[pred]}\")\n",
        "    show_img_and_heatmap(p, cam_map, alpha=0.45)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "wd4S6KCpDSie",
        "outputId": "a87f645c-ab94-47a2-da9b-f31b957074c1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "# Define class names\n",
        "class_names = ['Au', 'Tp']  # 0 -> Authentic, 1 -> Tampered\n",
        "\n",
        "# 1Ô∏è‚É£ Load best checkpoint\n",
        "best_ckpt = sorted(Path(\"/content/checkpoints_casia_vit\").glob(\"best_vit_epoch*_f1*.pt\"))[-1]\n",
        "checkpoint = torch.load(best_ckpt, map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "print(f\"‚úÖ Loaded model from: {best_ckpt}\")\n",
        "\n",
        "# 2Ô∏è‚É£ Function to predict and visualize\n",
        "def predict_image(img_path):\n",
        "    # Load image\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    img_tensor = val_transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img_tensor)\n",
        "        probs = torch.softmax(outputs, dim=1)[0]\n",
        "        pred_idx = probs.argmax().item()\n",
        "        confidence = probs[pred_idx].item()\n",
        "\n",
        "    print(f\"Prediction: {class_names[pred_idx]} ({confidence*100:.2f}% confidence)\")\n",
        "\n",
        "    # Compute Grad-CAM\n",
        "    cam_map, _ = compute_vit_gradcam(model, img_tensor)\n",
        "\n",
        "    # Visualization\n",
        "    show_img_and_heatmap(img_path, cam_map, alpha=0.45)\n",
        "\n",
        "# 3Ô∏è‚É£ Upload or use a path\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "    print(f\"\\nüîé Analyzing {fn} ...\")\n",
        "    predict_image(fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pF1IYyYmI9-p"
      },
      "outputs": [],
      "source": [
        "# # ===============================\n",
        "# # Grad-CAM debug & visualization cell (timm 1.0.21, ViT-B/16)\n",
        "# # Run this AFTER training and after you loaded the best checkpoint into `model`.\n",
        "# # It requires: model, device, val_transform, val_paths, val_labels\n",
        "# # ===============================\n",
        "# import torch, cv2, random, numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from PIL import Image\n",
        "# from IPython.display import display\n",
        "\n",
        "# plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "# # ---------- utility: show image / heatmap ----------\n",
        "# def show_img_and_heatmap_inline(img_path, cam_map, alpha=0.45, title=None):\n",
        "#     img = cv2.imread(img_path)\n",
        "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "#     img = cv2.resize(img, (img.shape[1], img.shape[0]))  # keep original aspect\n",
        "#     # resize cam_map to displayed size (use img size)\n",
        "#     cam_resized = cv2.resize(cam_map, (img.shape[1], img.shape[0]))\n",
        "#     heatmap = cv2.applyColorMap(np.uint8(255*cam_resized), cv2.COLORMAP_JET)\n",
        "#     heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "#     overlay = np.uint8(heatmap*alpha + img*(1-alpha))\n",
        "#     fig, ax = plt.subplots(1,3, figsize=(12,4))\n",
        "#     ax[0].imshow(img); ax[0].set_title(\"Image\"); ax[0].axis('off')\n",
        "#     ax[1].imshow(heatmap); ax[1].set_title(\"Heatmap\"); ax[1].axis('off')\n",
        "#     ax[2].imshow(overlay); ax[2].set_title(\"Overlay\"); ax[2].axis('off')\n",
        "#     if title:\n",
        "#         plt.suptitle(title)\n",
        "#     plt.show()\n",
        "\n",
        "# # ---------- helper: register hooks on a layer object ----------\n",
        "# def make_hooks(target_module):\n",
        "#     activations = {'val': None}\n",
        "#     gradients = {'val': None}\n",
        "#     def forward_hook(module, input, output):\n",
        "#         # output may be tensor or tuple; keep tensor\n",
        "#         out = output if isinstance(output, torch.Tensor) else output[0]\n",
        "#         activations['val'] = out\n",
        "#         # ensure requires_grad if needed for backward\n",
        "#         try:\n",
        "#             activations['val'].requires_grad_(True)\n",
        "#         except Exception:\n",
        "#             pass\n",
        "#     def backward_hook(module, grad_in, grad_out):\n",
        "#         g = grad_out[0] if isinstance(grad_out, tuple) else grad_out\n",
        "#         gradients['val'] = g\n",
        "#     fh = target_module.register_forward_hook(forward_hook)\n",
        "#     bh = target_module.register_full_backward_hook(backward_hook)\n",
        "#     return activations, gradients, fh, bh\n",
        "\n",
        "# # ---------- robust grad-cam function that handles qkv/proj/attn outputs ----------\n",
        "# def compute_gradcam_from_hooks(activations, gradients, input_tensor):\n",
        "#     \"\"\"\n",
        "#     activations['val'], gradients['val'] expected to exist and be Tensors.\n",
        "#     Handles cases:\n",
        "#       - activations shape: (tokens, C) where C may be 768 or 2304 (qkv concat).\n",
        "#       - gradients same shape.\n",
        "#     Returns cam_map resized to (H,W) in [0,1].\n",
        "#     \"\"\"\n",
        "#     acts = activations['val']   # shape (1, tokens, C) or (tokens, C)\n",
        "#     grads = gradients['val']\n",
        "#     if acts is None or grads is None:\n",
        "#         raise RuntimeError(\"Activations or gradients are None\")\n",
        "#     # align shapes to (tokens, C)\n",
        "#     if acts.dim() == 3:\n",
        "#         acts = acts[0]\n",
        "#     if grads.dim() == 3:\n",
        "#         grads = grads[0]\n",
        "#     # If channels are qkv concat (C == 3*embed_dim), split and use 'value' part if possible.\n",
        "#     C = acts.shape[1]\n",
        "#     if C % 3 == 0 and C >= 768:\n",
        "#         embed = C // 3\n",
        "#         # Use value component (3rd chunk)\n",
        "#         v = acts[:, 2*embed:3*embed]\n",
        "#         gv = grads[:, 2*embed:3*embed]\n",
        "#         acts_patches = v\n",
        "#         grads_patches = gv\n",
        "#     else:\n",
        "#         # default: use entire channels\n",
        "#         acts_patches = acts\n",
        "#         grads_patches = grads\n",
        "#     # remove CLS token if present\n",
        "#     if acts_patches.shape[0] > 1:\n",
        "#         acts_patches = acts_patches[1:]\n",
        "#         grads_patches = grads_patches[1:]\n",
        "#     # compute weights and CAM\n",
        "#     weights = grads_patches.mean(dim=0)            # (C,) importance per channel\n",
        "#     cam_patches = (acts_patches * weights.unsqueeze(0)).sum(dim=1).detach().cpu().numpy()  # (N_patches,)\n",
        "#     # reshape to grid\n",
        "#     n_patches = cam_patches.shape[0]\n",
        "#     grid_size = int(np.round(np.sqrt(n_patches)))\n",
        "#     if grid_size*grid_size != n_patches:\n",
        "#         # fallback: try to crop/pad to nearest square\n",
        "#         gs = grid_size\n",
        "#         n_needed = gs*gs\n",
        "#         if n_needed > n_patches:\n",
        "#             # pad with zeros\n",
        "#             cam_patches = np.pad(cam_patches, (0, n_needed - n_patches), 'constant')\n",
        "#         else:\n",
        "#             cam_patches = cam_patches[:n_needed]\n",
        "#         grid_size = gs\n",
        "#     cam_map = cam_patches.reshape(grid_size, grid_size)\n",
        "#     cam_map = np.maximum(cam_map, 0)\n",
        "#     cam_map -= cam_map.min()\n",
        "#     if cam_map.max() != 0:\n",
        "#         cam_map /= cam_map.max()\n",
        "#     # final map will be resized by caller to image size\n",
        "#     return cam_map\n",
        "\n",
        "# # ---------- Try multiple candidate layers and visualize results ----------\n",
        "# def try_layers_and_show(image_path, candidate_layer_names=None, top_k=3, alpha=0.45):\n",
        "#     \"\"\"\n",
        "#     candidate_layer_names: list of strings pointing to elements in model, e.g.\n",
        "#       - 'blocks[-1].attn.qkv'  (may not exist as tensor output)\n",
        "#       - 'blocks[-1].attn'      (attention module output)\n",
        "#       - 'blocks[-1].attn.proj'\n",
        "#       - 'norm'\n",
        "#       - 'blocks[-1]'\n",
        "#     The function locates attributes automatically and tries them.\n",
        "#     \"\"\"\n",
        "#     if candidate_layer_names is None:\n",
        "#         candidate_layer_names = [\n",
        "#             'blocks[-1].attn.qkv',\n",
        "#             'blocks[-1].attn',\n",
        "#             'blocks[-1].attn.proj',\n",
        "#             'blocks[-1].norm1',\n",
        "#             'blocks[-1].norm'\n",
        "#         ]\n",
        "#     # prepare input tensor\n",
        "#     img = Image.open(image_path).convert(\"RGB\")\n",
        "#     img_t = val_transform(img).unsqueeze(0).to(device)\n",
        "#     # get model output (for pred)\n",
        "#     with torch.no_grad():\n",
        "#         logits = model(img_t)\n",
        "#         probs = torch.softmax(logits, dim=1)[0].cpu().numpy()\n",
        "#         pred_idx = int(np.argmax(probs))\n",
        "#     print(f\"Model prediction: {pred_idx} ({probs[pred_idx]:.3f})\")\n",
        "#     results = []\n",
        "#     handles = []\n",
        "#     for name in candidate_layer_names:\n",
        "#         # try to resolve attribute path\n",
        "#         try:\n",
        "#             # support negative index like blocks[-1]\n",
        "#             parts = name.split('.')\n",
        "#             obj = model\n",
        "#             for p in parts:\n",
        "#                 if p.endswith(']'):\n",
        "#                     # handle index like blocks[-1]\n",
        "#                     base, idx = p[:-1].split('[')\n",
        "#                     idx = int(idx)\n",
        "#                     obj = getattr(obj, base)[idx]\n",
        "#                 else:\n",
        "#                     if hasattr(obj, p):\n",
        "#                         obj = getattr(obj, p)\n",
        "#                     else:\n",
        "#                         obj = obj  # leave as is; may be method etc.\n",
        "#             target = obj\n",
        "#         except Exception as e:\n",
        "#             # skip if resolution fails\n",
        "#             # print(f\"Skip layer {name}: {e}\")\n",
        "#             continue\n",
        "#         # register hooks\n",
        "#         try:\n",
        "#             activations, gradients, fh, bh = make_hooks(target)\n",
        "#         except Exception as e:\n",
        "#             # print(f\"Could not hook {name}: {e}\")\n",
        "#             continue\n",
        "#         # forward+backward to collect grads\n",
        "#         model.zero_grad()\n",
        "#         logits = model(img_t)\n",
        "#         score = logits[0, pred_idx]\n",
        "#         score.backward(retain_graph=True)\n",
        "#         # diagnostics\n",
        "#         a = activations['val']\n",
        "#         g = gradients['val']\n",
        "#         a_mean = float(a.abs().mean().detach().cpu().numpy()) if a is not None else None\n",
        "#         g_mean = float(g.abs().mean().detach().cpu().numpy()) if g is not None else None\n",
        "#         print(f\"Layer '{name}': act_mean={a_mean:.6f}, grad_mean={g_mean:.6g}\")\n",
        "#         # if grads extremely small, still try but note it\n",
        "#         try:\n",
        "#             cam = compute_gradcam_from_hooks(activations, gradients, img_t)\n",
        "#         except Exception as e:\n",
        "#             cam = None\n",
        "#             print(f\"  -> compute failed for {name}: {e}\")\n",
        "#         results.append((name, cam, a_mean, g_mean))\n",
        "#         # remove hooks\n",
        "#         fh.remove(); bh.remove()\n",
        "#     # show top_k visually (non-None cams)\n",
        "#     shown = 0\n",
        "#     for name, cam, a_mean, g_mean in results:\n",
        "#         if cam is None:\n",
        "#             continue\n",
        "#         print(f\"\\nShowing layer {name} (act_mean={a_mean:.4f}, grad_mean={g_mean:.6g})\")\n",
        "#         show_img_and_heatmap_inline(image_path, cam, alpha=alpha, title=f\"{name} act={a_mean:.3f} grad={g_mean:.6g}\")\n",
        "#         shown += 1\n",
        "#         if shown >= top_k:\n",
        "#             break\n",
        "#     if shown == 0:\n",
        "#         print(\"No usable CAMs produced. Try training more / different layers / verify gradients.\")\n",
        "\n",
        "# # ---------- small helper: single image predict + best-layer CAM ----------\n",
        "# def predict_and_show(image_path, chosen_layer_name='blocks[-1].attn'):\n",
        "#     # hook chosen layer and show single cam\n",
        "#     parts = chosen_layer_name.split('.')\n",
        "#     obj = model\n",
        "#     for p in parts:\n",
        "#         if p.endswith(']'):\n",
        "#             base, idx = p[:-1].split('[')\n",
        "#             idx = int(idx)\n",
        "#             obj = getattr(obj, base)[idx]\n",
        "#         else:\n",
        "#             obj = getattr(obj, p)\n",
        "#     activations, gradients, fh, bh = make_hooks(obj)\n",
        "#     img = Image.open(image_path).convert(\"RGB\")\n",
        "#     img_t = val_transform(img).unsqueeze(0).to(device)\n",
        "#     with torch.no_grad():\n",
        "#         logits = model(img_t)\n",
        "#         probs = torch.softmax(logits, dim=1)[0].cpu().numpy()\n",
        "#         pred_idx = int(np.argmax(probs))\n",
        "#     # compute cam (need backward so remove no_grad)\n",
        "#     model.zero_grad()\n",
        "#     logits = model(img_t)\n",
        "#     score = logits[0, pred_idx]\n",
        "#     score.backward(retain_graph=True)\n",
        "#     a_mean = float(activations['val'].abs().mean().detach().cpu().numpy())\n",
        "#     g_mean = float(gradients['val'].abs().mean().detach().cpu().numpy())\n",
        "#     cam = compute_gradcam_from_hooks(activations, gradients, img_t)\n",
        "#     fh.remove(); bh.remove()\n",
        "#     print(f\"Prediction: {pred_idx} ({probs[pred_idx]:.3f}), act_mean={a_mean:.6f}, grad_mean={g_mean:.6g}\")\n",
        "#     show_img_and_heatmap_inline(image_path, cam, alpha=0.45, title=f\"{chosen_layer_name}\")\n",
        "#     return pred_idx, probs[pred_idx]\n",
        "\n",
        "# # ---------- USAGE ----------\n",
        "# # Example: try automatic candidates on a single image (use any path from your val set)\n",
        "# example_image = val_paths[0]  # change index if you want another image\n",
        "# print(\"Testing image:\", example_image)\n",
        "# try_layers_and_show(example_image, candidate_layer_names=[\n",
        "#     'blocks[-1].attn.qkv',\n",
        "#     'blocks[-1].attn',\n",
        "#     'blocks[-1].attn.proj',\n",
        "#     'blocks[-1].norm1'\n",
        "# ], top_k=3, alpha=0.45)\n",
        "\n",
        "# # If one name shows a good heatmap, use:\n",
        "# # predict_and_show(example_image, chosen_layer_name='blocks[-1].attn.qkv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "IlGJGP5oQTvs",
        "outputId": "c8d83de3-3195-4d3c-e437-cd9a8cafade4"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# üîç Final ViT-B/16 Grad-CAM (for Deepfake/Tampered Detection)\n",
        "# =====================================================\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "\n",
        "# Class names (0 = Authentic, 1 = Tampered)\n",
        "class_names = ['Au', 'Tp']\n",
        "\n",
        "# ‚úÖ Use a mid-level transformer block for clearer spatial heatmaps\n",
        "chosen_layer_name = 'blocks[-3].attn'\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# Hook registration helpers\n",
        "# -----------------------------------------------------\n",
        "def make_hooks(target_module):\n",
        "    activations, gradients = {}, {}\n",
        "\n",
        "    def forward_hook(module, input, output):\n",
        "        out = output[0] if isinstance(output, tuple) else output\n",
        "        activations[\"val\"] = out\n",
        "        try:\n",
        "            activations[\"val\"].requires_grad_(True)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    def backward_hook(module, grad_in, grad_out):\n",
        "        g = grad_out[0] if isinstance(grad_out, tuple) else grad_out\n",
        "        gradients[\"val\"] = g\n",
        "\n",
        "    fh = target_module.register_forward_hook(forward_hook)\n",
        "    bh = target_module.register_full_backward_hook(backward_hook)\n",
        "    return activations, gradients, fh, bh\n",
        "\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# Grad-CAM computation\n",
        "# -----------------------------------------------------\n",
        "def compute_gradcam_from_hooks(activations, gradients, input_tensor):\n",
        "    acts = activations[\"val\"][0]   # (tokens, channels)\n",
        "    grads = gradients[\"val\"][0]\n",
        "    if acts.shape[0] > 1:  # drop CLS token\n",
        "        acts, grads = acts[1:], grads[1:]\n",
        "\n",
        "    weights = grads.mean(dim=0)\n",
        "    cam = (acts * weights.unsqueeze(0)).sum(dim=1).detach().cpu().numpy()\n",
        "    cam = np.maximum(cam, 0)\n",
        "\n",
        "    if cam.max() > 1e-6:\n",
        "        cam /= cam.max()\n",
        "    cam = cam ** 0.7  # gamma correction for better contrast\n",
        "\n",
        "    grid = int(np.sqrt(cam.shape[0]))\n",
        "    cam = cam.reshape(grid, grid)\n",
        "    cam = cv2.resize(cam, (input_tensor.shape[-1], input_tensor.shape[-2]))\n",
        "    return cam\n",
        "\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# Visualization helper\n",
        "# -----------------------------------------------------\n",
        "def show_img_and_heatmap(img_path, cam_map, alpha=0.45):\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    h, w, _ = img.shape\n",
        "\n",
        "    cam_resized = cv2.resize(cam_map, (w, h))\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * cam_resized), cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "    overlay = np.uint8(heatmap * alpha + img * (1 - alpha))\n",
        "\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
        "    ax[0].imshow(img); ax[0].set_title(\"Image\"); ax[0].axis(\"off\")\n",
        "    ax[1].imshow(heatmap); ax[1].set_title(\"Heatmap\"); ax[1].axis(\"off\")\n",
        "    ax[2].imshow(overlay); ax[2].set_title(\"Overlay\"); ax[2].axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# Prediction + Grad-CAM Visualization\n",
        "# -----------------------------------------------------\n",
        "def predict_and_visualize(image_path, chosen_layer_name='blocks[-3].attn'):\n",
        "    # Resolve the layer path dynamically\n",
        "    parts = chosen_layer_name.split('.')\n",
        "    obj = model\n",
        "    for p in parts:\n",
        "        if p.endswith(']'):\n",
        "            base, idx = p[:-1].split('[')\n",
        "            idx = int(idx)\n",
        "            obj = getattr(obj, base)[idx]\n",
        "        else:\n",
        "            obj = getattr(obj, p)\n",
        "\n",
        "    activations, gradients, fh, bh = make_hooks(obj)\n",
        "\n",
        "    # Preprocess image\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    img_t = val_transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    # Forward pass WITH gradients\n",
        "    outputs = model(img_t)\n",
        "    probs = torch.softmax(outputs, dim=1)[0]\n",
        "    pred_idx = int(probs.argmax())\n",
        "    confidence = probs[pred_idx].item()\n",
        "\n",
        "    # Backpropagate to compute Grad-CAM\n",
        "    model.zero_grad(set_to_none=True)\n",
        "    score = outputs[0, pred_idx]\n",
        "    score.backward(retain_graph=False)\n",
        "\n",
        "    cam_map = compute_gradcam_from_hooks(activations, gradients, img_t)\n",
        "    fh.remove(); bh.remove()\n",
        "\n",
        "    label = class_names[pred_idx].upper()\n",
        "    print(f\"\\nPrediction: {label} ({confidence*100:.2f}% confidence)\")\n",
        "    show_img_and_heatmap(image_path, cam_map, alpha=0.45)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# Upload and test new images\n",
        "# -----------------------------------------------------\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "    print(f\"\\nüîç Analyzing {fn} ...\")\n",
        "    predict_and_visualize(fn, chosen_layer_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YifsfDUbTdLE",
        "outputId": "b191617e-fbca-46eb-a749-551942f94ef7"
      },
      "outputs": [],
      "source": [
        "# =====================================================\n",
        "# üíæ Save trained model weights for inference\n",
        "# =====================================================\n",
        "save_path = \"/content/vit_casia_final.pth\"\n",
        "\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"‚úÖ Model weights saved successfully at: {save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "puehY68WTlKv",
        "outputId": "f7b2d86d-38d9-49da-c6ea-ef5ebd1840c7"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/vit_casia_final.pth\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "20d9040e22ef4d46998fe6e372f07184": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a0ac1fd1f51c49a2b0f9f9feaa0324b6",
              "IPY_MODEL_a8cd2d9f24964fe3b25829792dda9153",
              "IPY_MODEL_926e89829b15438bb88bf8d679641be1"
            ],
            "layout": "IPY_MODEL_775e993ee66d4c1197bffad63f8549aa"
          }
        },
        "36ae40ec55574fd190d0cd4a0e933c4e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65288f656f27476a9aaa6ca4d18e7dd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72db44d8e4f34215a44e1dbc4df41ae4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "775e993ee66d4c1197bffad63f8549aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "926e89829b15438bb88bf8d679641be1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36ae40ec55574fd190d0cd4a0e933c4e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_65288f656f27476a9aaa6ca4d18e7dd1",
            "value": "‚Äá346M/346M‚Äá[00:03&lt;00:00,‚Äá215MB/s]"
          }
        },
        "a0ac1fd1f51c49a2b0f9f9feaa0324b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd3e476217924e34892de50018034397",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d4db3e720e1f4018a3e517553a5ac21a",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "a8cd2d9f24964fe3b25829792dda9153": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f90d27bd74a34fe39f9409b06ba6548a",
            "max": 346284714,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_72db44d8e4f34215a44e1dbc4df41ae4",
            "value": 346284714
          }
        },
        "cd3e476217924e34892de50018034397": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4db3e720e1f4018a3e517553a5ac21a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f90d27bd74a34fe39f9409b06ba6548a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
